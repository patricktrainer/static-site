
    <!DOCTYPE html>
    <html>
        <head>
            <meta charset="utf-8">
            <meta name="viewport" content="width=device-width">
            <title>From â€œWhat is a Markov Modelâ€ to â€œHere is how Mark ec22e4d1d9d54a94b2f50cfe87242a8d</title>
            
            
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/open-fonts@1.1.1/fonts/inter.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@exampledev/new.css@1.1.2/new.min.css">
    <link rel="stylesheet" href="https://unpkg.com/github-syntax-dark@latest/lib/github-dark.css"/>
    
        </head>
        <body>
            <header>
                <nav>
                    <a href="https://patricktrainer.github.io/static-site">Home</a> /
                    <a href="https://github.com/patricktrainer">GitHub</a> / 
                </nav>
            </header>
            <h1><a id="user-content-from-what-is-a-markov-model-to-here-is-how-markov-models-work---by" class="anchor" aria-hidden="true" href="#from-what-is-a-markov-model-to-here-is-how-markov-models-work---by"><span aria-hidden="true" class="octicon octicon-link"></span></a>From â€œWhat is a Markov Modelâ€ to â€œHere is how Markov Models Workâ€ - By</h1>
<p>Created: Feb 21, 2020 12:22 PM
URL: <a href="https://hackernoon.com/from-what-is-a-markov-model-to-here-is-how-markov-models-work-1ac5f4629b71" rel="nofollow">https://hackernoon.com/from-what-is-a-markov-model-to-here-is-how-markov-models-work-1ac5f4629b71</a></p>
<p>To be honest, if you are just looking to answer the age old question of â€œwhat is a Markov Modelâ€ you should take a visit to Wikipedia (or just check the TLDR <g-emoji class="g-emoji" alias="wink" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f609.png">ğŸ˜‰</g-emoji>), but if you are curious and looking to use some examples to aid in your understanding of <em>what</em> a Markov Model is, <em>why</em> Markov Models Matter, and <em>how to implement</em> a Markov Model stick around :) <strong>Show &gt; Tell</strong></p>
<blockquote>
<p>TLDR: â€œIn probability theory, a Markov model is a stochastic model used to model randomly changing systems where it is assumed that future states depend only on the current state not on the events that occurred before it (that is, it assumes the Markov property). Source: <a href="https://en.wikipedia.org/wiki/Markov_model" rel="nofollow">https://en.wikipedia.org/wiki/Markov_model</a></p>
</blockquote>
<p>Roadmaps are great! Check out this <strong>table of contents</strong> for this articleâ€™s roadmap <g-emoji class="g-emoji" alias="car" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f697.png">ğŸš—</g-emoji></p>
<h3><a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"><span aria-hidden="true" class="octicon octicon-link"></span></a>Table of Contents</h3>
<h3><a id="user-content-a-intro-to-markov-models-" class="anchor" aria-hidden="true" href="#a-intro-to-markov-models-"><span aria-hidden="true" class="octicon octicon-link"></span></a>A. Intro To Markov Models <g-emoji class="g-emoji" alias="baby" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f476.png">ğŸ‘¶</g-emoji></h3>
<ol>
<li>Starter Sentence</li>
<li>Weighted Distributions</li>
<li>Special Additions</li>
<li>
<em>How</em> a Markov Model Works</li>
<li>Full Example Summary</li>
</ol>
<h3><a id="user-content-b-further-markov-model-topics-" class="anchor" aria-hidden="true" href="#b-further-markov-model-topics-"><span aria-hidden="true" class="octicon octicon-link"></span></a>B. Further Markov Model Topics <g-emoji class="g-emoji" alias="smirk_cat" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f63c.png">ğŸ˜¼</g-emoji></h3>
<ol>
<li>Larger Example</li>
<li>Distribution</li>
<li>Bigger Windows</li>
</ol>
<h3><a id="user-content-c-implementation---python-" class="anchor" aria-hidden="true" href="#c-implementation---python-"><span aria-hidden="true" class="octicon octicon-link"></span></a>C. Implementation - Python <g-emoji class="g-emoji" alias="snake" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f40d.png">ğŸ</g-emoji></h3>
<ol>
<li>Dictogram Data Structure</li>
<li>Hash Table Data Structure</li>
<li>Markov Model Structure</li>
<li>Parse Markov Model</li>
</ol>
<h3><a id="user-content-d-further-readings-suggestions-thoughts-" class="anchor" aria-hidden="true" href="#d-further-readings-suggestions-thoughts-"><span aria-hidden="true" class="octicon octicon-link"></span></a>D. Further Readings, Suggestions, Thoughts <g-emoji class="g-emoji" alias="green_heart" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f49a.png">ğŸ’š</g-emoji></h3>
<ol>
<li>Applications</li>
<li>Further Reading</li>
<li>Final Thoughts</li>
</ol>
<h3><a id="user-content-intro-to-markov-models-ï¸--ï¸--ï¸---" class="anchor" aria-hidden="true" href="#intro-to-markov-models-ï¸--ï¸--ï¸---"><span aria-hidden="true" class="octicon octicon-link"></span></a>Intro To Markov Models (<g-emoji class="g-emoji" alias="point_up" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/261d.png">â˜ï¸</g-emoji> <g-emoji class="g-emoji" alias="tropical_fish" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f420.png">ğŸ </g-emoji> <g-emoji class="g-emoji" alias="v" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/270c.png">âœŒï¸</g-emoji> <g-emoji class="g-emoji" alias="tropical_fish" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f420.png">ğŸ </g-emoji> <g-emoji class="g-emoji" alias="o" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2b55.png">â­•ï¸</g-emoji> <g-emoji class="g-emoji" alias="tropical_fish" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f420.png">ğŸ </g-emoji> <g-emoji class="g-emoji" alias="cyclone" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f300.png">ğŸŒ€</g-emoji> <g-emoji class="g-emoji" alias="tropical_fish" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f420.png">ğŸ </g-emoji>)</h3>
<p><strong>1. Starter Sentence</strong> | Definitely the best way to illustrate Markov models is through using an example. In this case we are going to use the same example that I was also presented when learning about Markov Models at <a href="https://medium.com/@makeschool" rel="nofollow">Make School</a>.</p>
<blockquote>
<p>â€œOne fish two fish red fish blue fish.â€ -Dr. Seuss <g-emoji class="g-emoji" alias="tophat" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3a9.png">ğŸ©</g-emoji></p>
</blockquote>
<p><strong>2. Weighted Distributions</strong> | Before we jump into Markov models we need to make sure we have a strong understanding of the <strong>given starter sentence, weighted distributions,</strong> and <strong>histograms</strong>.</p>
<p><a target="_blank" rel="noopener noreferrer" href="From%20%E2%80%9CWhat%20is%20a%20Markov%20Model%E2%80%9D%20to%20%E2%80%9CHere%20is%20how%20Mark%20ec22e4d1d9d54a94b2f50cfe87242a8d/1Q20t6KQDq0GhrF-NCyqxHg.png"><img src="From%20%E2%80%9CWhat%20is%20a%20Markov%20Model%E2%80%9D%20to%20%E2%80%9CHere%20is%20how%20Mark%20ec22e4d1d9d54a94b2f50cfe87242a8d/1Q20t6KQDq0GhrF-NCyqxHg.png" alt="From%20%E2%80%9CWhat%20is%20a%20Markov%20Model%E2%80%9D%20to%20%E2%80%9CHere%20is%20how%20Mark%20ec22e4d1d9d54a94b2f50cfe87242a8d/1Q20t6KQDq0GhrF-NCyqxHg.png" style="max-width: 100%;"></a></p>
<p>Starter Sentence</p>
<p>Cool, our starter sentence is a well known phrase and on the surface nothing may explicitly jump out. But we are going to break it down and look at what <em>composes</em> this exact sentence. Specifically, it consists of eight words (<strong>tokens</strong>) but only five unique words (<strong>keys</strong>).</p>
<p><a target="_blank" rel="noopener noreferrer" href="From%20%E2%80%9CWhat%20is%20a%20Markov%20Model%E2%80%9D%20to%20%E2%80%9CHere%20is%20how%20Mark%20ec22e4d1d9d54a94b2f50cfe87242a8d/1GwzLn-9tMkRI4dRclS3now.png"><img src="From%20%E2%80%9CWhat%20is%20a%20Markov%20Model%E2%80%9D%20to%20%E2%80%9CHere%20is%20how%20Mark%20ec22e4d1d9d54a94b2f50cfe87242a8d/1GwzLn-9tMkRI4dRclS3now.png" alt="From%20%E2%80%9CWhat%20is%20a%20Markov%20Model%E2%80%9D%20to%20%E2%80%9CHere%20is%20how%20Mark%20ec22e4d1d9d54a94b2f50cfe87242a8d/1GwzLn-9tMkRI4dRclS3now.png" style="max-width: 100%;"></a></p>
<p>Colored Starter Sentence</p>
<p>Here I gave each unique word (<strong>key</strong>) a different color and on the surface this is now just a colored sentenceâ€¦but alas, there is more meaning behind coloring each <strong>key</strong> differently. By coloring each unique <strong>key</strong> differently we can see that certain <strong>keys</strong> appear much more often than others.</p>
<p><a target="_blank" rel="noopener noreferrer" href="From%20%E2%80%9CWhat%20is%20a%20Markov%20Model%E2%80%9D%20to%20%E2%80%9CHere%20is%20how%20Mark%20ec22e4d1d9d54a94b2f50cfe87242a8d/1gnwXq22M3L3-bBXibqXqaw.png"><img src="From%20%E2%80%9CWhat%20is%20a%20Markov%20Model%E2%80%9D%20to%20%E2%80%9CHere%20is%20how%20Mark%20ec22e4d1d9d54a94b2f50cfe87242a8d/1gnwXq22M3L3-bBXibqXqaw.png" alt="From%20%E2%80%9CWhat%20is%20a%20Markov%20Model%E2%80%9D%20to%20%E2%80%9CHere%20is%20how%20Mark%20ec22e4d1d9d54a94b2f50cfe87242a8d/1gnwXq22M3L3-bBXibqXqaw.png" style="max-width: 100%;"></a></p>
<p>Distribution of keys</p>
<p>By looking at the above <strong>distribution</strong> of <strong>keys</strong> we could deduce that the <strong>key</strong> fish comes up 4x as much as any other <strong>key.</strong> This type of statement can led us to even further predictions such as if I randomly had to pick the next word at any point in the starter sentence my best guess would be saying â€œfishâ€ because it occurs significantly more in the sentence than any other word. <g-emoji class="g-emoji" alias="100" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4af.png">ğŸ’¯</g-emoji>
In our situation <strong>weighted distributions</strong> are the percentage that one key will appear is based on the total amount of times a key shows up divided by the total amount of tokens. For example, the weighted distribution for fish is 50% because it occurs 4 times out of the total 8 words. Then One, two, red, blue all have a 12.5% chance of occurring (1/8 each).</p>
<p><strong>Histograms</strong> are a way to represent weighted distributions, often they are a plot that enables you to discover the underlying frequency distribution of a set of continuous data. In our case the continuous data is a <em>sentence</em> because a sentence consists of many words (continuous data).</p>
<p><a target="_blank" rel="noopener noreferrer" href="From%20%E2%80%9CWhat%20is%20a%20Markov%20Model%E2%80%9D%20to%20%E2%80%9CHere%20is%20how%20Mark%20ec22e4d1d9d54a94b2f50cfe87242a8d/1sqA96Aytk6GG1IIMlPR_EA.png"><img src="From%20%E2%80%9CWhat%20is%20a%20Markov%20Model%E2%80%9D%20to%20%E2%80%9CHere%20is%20how%20Mark%20ec22e4d1d9d54a94b2f50cfe87242a8d/1sqA96Aytk6GG1IIMlPR_EA.png" alt="From%20%E2%80%9CWhat%20is%20a%20Markov%20Model%E2%80%9D%20to%20%E2%80%9CHere%20is%20how%20Mark%20ec22e4d1d9d54a94b2f50cfe87242a8d/1sqA96Aytk6GG1IIMlPR_EA.png" style="max-width: 100%;"></a></p>
<p>Histogram for our starter sentence</p>
<p>By looking at the <strong>histogram</strong> of our starter sentence we can see the underlying distribution of words <strong>visually <g-emoji class="g-emoji" alias="eyes" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f440.png">ğŸ‘€</g-emoji></strong> Clearly, fish appears more than anything else in our data set <g-emoji class="g-emoji" alias="tropical_fish" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f420.png">ğŸ </g-emoji><g-emoji class="g-emoji" alias="fish" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f41f.png">ğŸŸ</g-emoji> <g-emoji class="g-emoji" alias="blowfish" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f421.png">ğŸ¡</g-emoji><g-emoji class="g-emoji" alias="tropical_fish" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f420.png">ğŸ </g-emoji></p>
<p><strong>3. Special Additions</strong> | Great! At this point you should be comfortable with the concept that our <em>sentence</em> consists of many <strong>tokens</strong> and <strong>keys.</strong> Additionally, you should understand the relationship between a <strong>histogram</strong> and <strong>weighted distributions</strong>.</p>
<p><g-emoji class="g-emoji" alias="white_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2705.png">âœ…</g-emoji><g-emoji class="g-emoji" alias="100" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4af.png">ğŸ’¯</g-emoji><g-emoji class="g-emoji" alias="white_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2705.png">âœ…</g-emoji> <strong>Extra Example</strong> <g-emoji class="g-emoji" alias="white_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2705.png">âœ…</g-emoji><g-emoji class="g-emoji" alias="100" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4af.png">ğŸ’¯</g-emoji><g-emoji class="g-emoji" alias="white_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2705.png">âœ…</g-emoji></p>
<p>A <strong>token</strong> is any word in the sentence.
A <strong>key</strong> is a unique occurrence of a word.**Example: â€œ**<em>Fish Fish Fish Fish Catâ€</em> there are two <strong>keys</strong> and five <strong>tokens.</strong> The keys are â€œFishâ€ and â€œCatâ€ (<g-emoji class="g-emoji" alias="tropical_fish" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f420.png">ğŸ </g-emoji> and <g-emoji class="g-emoji" alias="smiley_cat" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f63a.png">ğŸ˜º</g-emoji>). Then any word is a token.
A <strong>histogram</strong> is related to <strong>weighted distibutions</strong> because a histogram visually shows the frequency of data in a continuous data set and in essence that is demonstrating the weighted distribution of the data.</p>
<p><g-emoji class="g-emoji" alias="white_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2705.png">âœ…</g-emoji><g-emoji class="g-emoji" alias="100" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4af.png">ğŸ’¯</g-emoji><g-emoji class="g-emoji" alias="white_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2705.png">âœ…</g-emoji> <strong>End</strong> <g-emoji class="g-emoji" alias="white_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2705.png">âœ…</g-emoji><g-emoji class="g-emoji" alias="100" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4af.png">ğŸ’¯</g-emoji><g-emoji class="g-emoji" alias="white_check_mark" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2705.png">âœ…</g-emoji></p>
<p>Cool, so now we understand our sentence at the surface and how certain words occur more than others But before we continue we need to add some special additions to our sentence that are hidden on the surface but we can agree are there. The <strong>Start</strong> and <strong>End</strong> of the sentenceâ€¦</p>
<p><a target="_blank" rel="noopener noreferrer" href="From%20%E2%80%9CWhat%20is%20a%20Markov%20Model%E2%80%9D%20to%20%E2%80%9CHere%20is%20how%20Mark%20ec22e4d1d9d54a94b2f50cfe87242a8d/1U06RW22yqWz8tK9gMeQ3MA.png"><img src="From%20%E2%80%9CWhat%20is%20a%20Markov%20Model%E2%80%9D%20to%20%E2%80%9CHere%20is%20how%20Mark%20ec22e4d1d9d54a94b2f50cfe87242a8d/1U06RW22yqWz8tK9gMeQ3MA.png" alt="From%20%E2%80%9CWhat%20is%20a%20Markov%20Model%E2%80%9D%20to%20%E2%80%9CHere%20is%20how%20Mark%20ec22e4d1d9d54a94b2f50cfe87242a8d/1U06RW22yqWz8tK9gMeQ3MA.png" style="max-width: 100%;"></a></p>
<p>Starter Sentence with Special Additions</p>
<p>Awesome! Take a moment and check out the above â€œadditionsâ€ to the sentence that exist. This may seem unnecessary right now, but trust me, this will make exponentially more sense in the next part where we dive into Markov models <g-emoji class="g-emoji" alias="relieved" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f60c.png">ğŸ˜Œ</g-emoji>. In summary, every sentence proceeds by an invisible â€œ<em>START</em>â€ symbol and it always concludes with an â€œ<em>END</em>â€ symbol.</p>
<p><a target="_blank" rel="noopener noreferrer" href="From%20%E2%80%9CWhat%20is%20a%20Markov%20Model%E2%80%9D%20to%20%E2%80%9CHere%20is%20how%20Mark%20ec22e4d1d9d54a94b2f50cfe87242a8d/1okyW8itkBNhiugBqZX3k0A.png"><img src="From%20%E2%80%9CWhat%20is%20a%20Markov%20Model%E2%80%9D%20to%20%E2%80%9CHere%20is%20how%20Mark%20ec22e4d1d9d54a94b2f50cfe87242a8d/1okyW8itkBNhiugBqZX3k0A.png" alt="From%20%E2%80%9CWhat%20is%20a%20Markov%20Model%E2%80%9D%20to%20%E2%80%9CHere%20is%20how%20Mark%20ec22e4d1d9d54a94b2f50cfe87242a8d/1okyW8itkBNhiugBqZX3k0A.png" style="max-width: 100%;"></a></p>
<p>Distribution of Keys in Modified Example</p>
<p>Above, I went ahead and recreated the same distribution of keys from earlier but included our two additional keys (<em>START</em> and <em>END</em>).</p>
<p><strong>4. <em>How</em> a Markov Model Works</strong> | Fantastic! You already may have learned a few things, but now here comes the meat of the article. Lets start from a high level definition of <em>What</em> a Markov Model is (according to <a href="https://en.wikipedia.org/wiki/Markov_model" rel="nofollow">Wikipedia</a>):</p>
<blockquote>
<p>â€œA Markov model is a stochastic model used to model randomly changing systems where it is assumed that future states depend only on the current state not on the events that occurred before it (that is, it assumes the Markov property). Generally, this assumption enables reasoning and computation with the model that would otherwise be intractable.â€-Wikipedia</p>
</blockquote>
<p>Awesome! Sounds interestingâ€¦but what does that huge blob even mean? I bolded the <strong>critical</strong> portion of <em>what</em> a Markov Model is. In summary, a Markov Model is a model where the <strong>next state</strong> is solely chosen based on the <strong>current state</strong>.</p>
<p>One way to think about it is you have a window that only shows the current state (or in our case a single token) and then you have to determine what the next token is based on that small window!</p>
<p><a target="_blank" rel="noopener noreferrer" href="From%20%E2%80%9CWhat%20is%20a%20Markov%20Model%E2%80%9D%20to%20%E2%80%9CHere%20is%20how%20Mark%20ec22e4d1d9d54a94b2f50cfe87242a8d/1xZwT88rLbYZhvAkx-XbOsw.png"><img src="From%20%E2%80%9CWhat%20is%20a%20Markov%20Model%E2%80%9D%20to%20%E2%80%9CHere%20is%20how%20Mark%20ec22e4d1d9d54a94b2f50cfe87242a8d/1xZwT88rLbYZhvAkx-XbOsw.png" alt="From%20%E2%80%9CWhat%20is%20a%20Markov%20Model%E2%80%9D%20to%20%E2%80%9CHere%20is%20how%20Mark%20ec22e4d1d9d54a94b2f50cfe87242a8d/1xZwT88rLbYZhvAkx-XbOsw.png" style="max-width: 100%;"></a></p>
<p>Demonstrating the next word using a window</p>
<p>Above, I showed how each token leads to another token. Additionally, I colored the arrow leading to the next word based on the origin key. I recommend you spend some time on this diagram and the following ones because they build the foundation of <em>how</em> Markov Models work! <g-emoji class="g-emoji" alias="nerd_face" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f913.png">ğŸ¤“</g-emoji></p>
<p><a target="_blank" rel="noopener noreferrer" href="From%20%E2%80%9CWhat%20is%20a%20Markov%20Model%E2%80%9D%20to%20%E2%80%9CHere%20is%20how%20Mark%20ec22e4d1d9d54a94b2f50cfe87242a8d/110vdH6xwkhX93ETi9ATsIw.png"><img src="From%20%E2%80%9CWhat%20is%20a%20Markov%20Model%E2%80%9D%20to%20%E2%80%9CHere%20is%20how%20Mark%20ec22e4d1d9d54a94b2f50cfe87242a8d/110vdH6xwkhX93ETi9ATsIw.png" alt="From%20%E2%80%9CWhat%20is%20a%20Markov%20Model%E2%80%9D%20to%20%E2%80%9CHere%20is%20how%20Mark%20ec22e4d1d9d54a94b2f50cfe87242a8d/110vdH6xwkhX93ETi9ATsIw.png" style="max-width: 100%;"></a></p>
<p>Pairs! Every Token leads to another Token!</p>
<p>You may have noticed that every token leads to another one (even the <em>END</em>, leads to another token â€” <em>none</em>). In this case it forms pairs of one token to another token!</p>
<p><a target="_blank" rel="noopener noreferrer" href="From%20%E2%80%9CWhat%20is%20a%20Markov%20Model%E2%80%9D%20to%20%E2%80%9CHere%20is%20how%20Mark%20ec22e4d1d9d54a94b2f50cfe87242a8d/1UGdrm8lZmk1Lp3OxguaYWA.png"><img src="From%20%E2%80%9CWhat%20is%20a%20Markov%20Model%E2%80%9D%20to%20%E2%80%9CHere%20is%20how%20Mark%20ec22e4d1d9d54a94b2f50cfe87242a8d/1UGdrm8lZmk1Lp3OxguaYWA.png" alt="From%20%E2%80%9CWhat%20is%20a%20Markov%20Model%E2%80%9D%20to%20%E2%80%9CHere%20is%20how%20Mark%20ec22e4d1d9d54a94b2f50cfe87242a8d/1UGdrm8lZmk1Lp3OxguaYWA.png" style="max-width: 100%;"></a></p>
<p>Organized pairs by starting word</p>
<p>Above, I simply organized the pairs by their first token. At this point you may be recognizing something interesting <g-emoji class="g-emoji" alias="thinking" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f914.png">ğŸ¤”</g-emoji> Each starting token is followed <strong>only</strong> by a possible key to follow itâ€¦</p>
<p><a target="_blank" rel="noopener noreferrer" href="From%20%E2%80%9CWhat%20is%20a%20Markov%20Model%E2%80%9D%20to%20%E2%80%9CHere%20is%20how%20Mark%20ec22e4d1d9d54a94b2f50cfe87242a8d/1QEIDJPVBcct-Jld6cDdUKQ.png"><img src="From%20%E2%80%9CWhat%20is%20a%20Markov%20Model%E2%80%9D%20to%20%E2%80%9CHere%20is%20how%20Mark%20ec22e4d1d9d54a94b2f50cfe87242a8d/1QEIDJPVBcct-Jld6cDdUKQ.png" alt="From%20%E2%80%9CWhat%20is%20a%20Markov%20Model%E2%80%9D%20to%20%E2%80%9CHere%20is%20how%20Mark%20ec22e4d1d9d54a94b2f50cfe87242a8d/1QEIDJPVBcct-Jld6cDdUKQ.png" style="max-width: 100%;"></a></p>
<p>Possible tokens to follow each key</p>
<p>Ok, so hopefully you have followed along and understood that we are organizing pairs which we formed by using a <strong>â€œwindowâ€</strong> to look at what the next token is in a pair. Then above I trimmed the pairs down even further into something very interesting. Every <strong>key</strong> is matched with an array of possible tokens that could follow that key.</p>
<p><g-emoji class="g-emoji" alias="sparkles" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png">âœ¨</g-emoji><g-emoji class="g-emoji" alias="bulb" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4a1.png">ğŸ’¡</g-emoji><g-emoji class="g-emoji" alias="sparkles" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png">âœ¨</g-emoji> <strong>Thinking Break</strong> <g-emoji class="g-emoji" alias="sparkles" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png">âœ¨</g-emoji><g-emoji class="g-emoji" alias="bulb" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4a1.png">ğŸ’¡</g-emoji><g-emoji class="g-emoji" alias="sparkles" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png">âœ¨</g-emoji></p>
<p>Letâ€™s take a moment to think about the above diagram. Every <strong>key</strong> has possible words that <em><strong>could</strong></em> follow it. If we were to give this structure from above to someone they could potentially recreate our original sentence!</p>
<h3><a id="user-content-example" class="anchor" aria-hidden="true" href="#example"><span aria-hidden="true" class="octicon octicon-link"></span></a><strong>Example:</strong></h3>
<p>We give them <em><strong>Start</strong></em> to begin with, then we look at the potential options of words that <em><strong>could</strong></em> follow <em>START</em> â†’ [One]. Being that there is only key that follows we have to pick it. Our sentence now looks like â€œOne.â€ Letâ€™s continue by looking at the potential words that <em><strong>could</strong></em> follow â€œOneâ€ â†’ [fish]. Well again, that was easy only â€œfishâ€ can follow One. Now our sentence is â€œOne fish.â€ Now letâ€™s see what <em><strong>could</strong></em> follow â€œfishâ€ â†’ [two, red, blue, <em>END</em>]. Here is where things get interesting any of these four options <em><strong>could</strong></em> be picked next <g-emoji class="g-emoji" alias="flushed" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f633.png">ğŸ˜³</g-emoji>. Which means we <em><strong>could</strong></em> pick â€œtwoâ€ and then continue and potentially get our original sentenceâ€¦but there is a 25% (1/4) chance we just randomly pick â€œ<em>END</em>â€. If this was the case we would have used our original structure and randomly generated a sentence very different than our original â†’ â€œOne fish.â€ <g-emoji class="g-emoji" alias="one" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/0031-20e3.png">1ï¸âƒ£</g-emoji> <g-emoji class="g-emoji" alias="tropical_fish" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f420.png">ğŸ </g-emoji></p>
<p><g-emoji class="g-emoji" alias="sparkles" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png">âœ¨</g-emoji><g-emoji class="g-emoji" alias="bulb" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4a1.png">ğŸ’¡</g-emoji><g-emoji class="g-emoji" alias="sparkles" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png">âœ¨</g-emoji> <strong>End</strong> <g-emoji class="g-emoji" alias="sparkles" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png">âœ¨</g-emoji><g-emoji class="g-emoji" alias="bulb" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4a1.png">ğŸ’¡</g-emoji><g-emoji class="g-emoji" alias="sparkles" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png">âœ¨</g-emoji></p>
<p>Congrats! <g-emoji class="g-emoji" alias="tada" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f389.png">ğŸ‰</g-emoji><g-emoji class="g-emoji" alias="tada" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f389.png">ğŸ‰</g-emoji> You secretly just acted out a Markov Model in the above <strong>Thinking Break</strong> <g-emoji class="g-emoji" alias="smirk" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f60f.png">ğŸ˜</g-emoji>. Nice! But seriouslyâ€¦think about it. We used the <strong>current state</strong> (current key) to determine our <strong>next state</strong>. Further our next state could only be a key that follows the current key. Sounds cool, but it gets even cooler! Letâ€™s diagram a Markov Model for our starter sentence.</p>
<p><a target="_blank" rel="noopener noreferrer" href="From%20%E2%80%9CWhat%20is%20a%20Markov%20Model%E2%80%9D%20to%20%E2%80%9CHere%20is%20how%20Mark%20ec22e4d1d9d54a94b2f50cfe87242a8d/1x4CnHyAqGkPa7ews_5f0sQ.png"><img src="From%20%E2%80%9CWhat%20is%20a%20Markov%20Model%E2%80%9D%20to%20%E2%80%9CHere%20is%20how%20Mark%20ec22e4d1d9d54a94b2f50cfe87242a8d/1x4CnHyAqGkPa7ews_5f0sQ.png" alt="From%20%E2%80%9CWhat%20is%20a%20Markov%20Model%E2%80%9D%20to%20%E2%80%9CHere%20is%20how%20Mark%20ec22e4d1d9d54a94b2f50cfe87242a8d/1x4CnHyAqGkPa7ews_5f0sQ.png" style="max-width: 100%;"></a></p>
<p>Markov Model for the Starter Sentence</p>
<p>Yikes <g-emoji class="g-emoji" alias="upside_down_face" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f643.png">ğŸ™ƒ</g-emoji> How does the above diagram represent what we just did? Look closely, each <em>oval</em> with a word inside it represents a <strong>key</strong> with the <em>arrows</em> pointing to potential <strong>keys</strong> that can follow it! But wait it gets even cooler:</p>
<p><a target="_blank" rel="noopener noreferrer" href="From%20%E2%80%9CWhat%20is%20a%20Markov%20Model%E2%80%9D%20to%20%E2%80%9CHere%20is%20how%20Mark%20ec22e4d1d9d54a94b2f50cfe87242a8d/1UuD1-B7CFn3M2nUDy02sHg.png"><img src="From%20%E2%80%9CWhat%20is%20a%20Markov%20Model%E2%80%9D%20to%20%E2%80%9CHere%20is%20how%20Mark%20ec22e4d1d9d54a94b2f50cfe87242a8d/1UuD1-B7CFn3M2nUDy02sHg.png" alt="From%20%E2%80%9CWhat%20is%20a%20Markov%20Model%E2%80%9D%20to%20%E2%80%9CHere%20is%20how%20Mark%20ec22e4d1d9d54a94b2f50cfe87242a8d/1UuD1-B7CFn3M2nUDy02sHg.png" style="max-width: 100%;"></a></p>
<p>Markov Model for the Starter Sentence with Probability</p>
<p>Yep! Each arrow has a probability that it will be selected to be the path that the current state will follow to the next state.</p>
<p><a target="_blank" rel="noopener noreferrer" href="From%20%E2%80%9CWhat%20is%20a%20Markov%20Model%E2%80%9D%20to%20%E2%80%9CHere%20is%20how%20Mark%20ec22e4d1d9d54a94b2f50cfe87242a8d/1MbHRwYNA8F29hzes8EPHiQ.gif"><img src="From%20%E2%80%9CWhat%20is%20a%20Markov%20Model%E2%80%9D%20to%20%E2%80%9CHere%20is%20how%20Mark%20ec22e4d1d9d54a94b2f50cfe87242a8d/1MbHRwYNA8F29hzes8EPHiQ.gif" alt="From%20%E2%80%9CWhat%20is%20a%20Markov%20Model%E2%80%9D%20to%20%E2%80%9CHere%20is%20how%20Mark%20ec22e4d1d9d54a94b2f50cfe87242a8d/1MbHRwYNA8F29hzes8EPHiQ.gif" style="max-width: 100%;"></a></p>
<p>Full Gif of the Starter Sentence being Created Using A Markov Model.</p>
<p>Awesome! In summary, we now understand and have illustrated a Markov Model by using the Dr. Seuss starter sentence. As a <strong>fun fact</strong>, the data you use to create your model is often referred to as a <strong>corpus <g-emoji class="g-emoji" alias="ghost" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f47b.png">ğŸ‘»</g-emoji></strong></p>
<p><strong>5. Full Example Summary</strong> | You made it! Congrats again <g-emoji class="g-emoji" alias="medal_sports" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3c5.png">ğŸ…</g-emoji> at this point you likely can describe what a Markov Model is and even possibly teach someone else how they work using this same basic example! You my friend are going places <g-emoji class="g-emoji" alias="rocket" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f680.png">ğŸš€</g-emoji>. But guess what! This was just the beginning of your fuller understanding of Markov Models in the following sections we will continue to grow and expand your understanding :) Remember distributions? Well we are going to use them in the next example to show how to use <strong>weighted distributions</strong> to <em>potentially</em> create a more accurate model; Further, we will talk about <strong>bigger windows</strong> <g-emoji class="g-emoji" alias="astonished" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f632.png">ğŸ˜²</g-emoji> (bigger is better, right? <g-emoji class="g-emoji" alias="roll_eyes" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f644.png">ğŸ™„</g-emoji>); and lastly we will implement a nifty Markov Model in Python <g-emoji class="g-emoji" alias="snake" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f40d.png">ğŸ</g-emoji>. So buckle up and enjoy the ride <g-emoji class="g-emoji" alias="fire" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png">ğŸ”¥</g-emoji><g-emoji class="g-emoji" alias="roller_coaster" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3a2.png">ğŸ¢</g-emoji></p>
<h3><a id="user-content-further-markov-model-topics-" class="anchor" aria-hidden="true" href="#further-markov-model-topics-"><span aria-hidden="true" class="octicon octicon-link"></span></a>Further Markov Model Topics <g-emoji class="g-emoji" alias="smirk_cat" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f63c.png">ğŸ˜¼</g-emoji></h3>
<p><strong>Disclaimer</strong> <g-emoji class="g-emoji" alias="fox_face" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f98a.png">ğŸ¦Š</g-emoji> I am going to be following the same process as above for creating the Markov Model, but I am going to omit some steps. If something appears confusing refer back to the first section <g-emoji class="g-emoji" alias="fire" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f525.png">ğŸ”¥</g-emoji></p>
<p><strong>1. Larger Example</strong> | Keeping in the spirit of Dr. Seuss quotes I went ahead and found four quotes that Theodor Seuss Geisel has <strong>immortalized:</strong></p>
<blockquote>
<p>â€œToday you are you. That is truer than true. There is no one alive who is you-er than you.â€</p>
</blockquote>
<blockquote>
<p>â€œYou have brains in your head. You have feet in your shoes. You can steer yourself any direction you choose. Youâ€™re on your own.â€</p>
</blockquote>
<blockquote>
<p>â€œThe more that you read, the more things you will know. The more that you learn, the more places youâ€™ll go.â€</p>
</blockquote>
<blockquote>
<p>â€œThink left and think right and think low and think high. Oh, the thinks you can think up if only you try.â€ -Dr. Seuss <g-emoji class="g-emoji" alias="tophat" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3a9.png">ğŸ©</g-emoji></p>
</blockquote>
<p>The biggest difference between the original starter sentence and our new sentence is the fact that some <strong>keys</strong> follow different keys a <strong>variable amount</strong> of times. For example â€œmoreâ€ follows â€œtheâ€ four times. So what will this additional complexity do to our Markov Model construction? <g-emoji class="g-emoji" alias="thinking" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f914.png">ğŸ¤”</g-emoji> Well overall it <strong>can improve</strong> our logical outcome for our sentences. What I mean by that is: There are certain words in the english language (or any language for that matter <g-emoji class="g-emoji" alias="romania" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f1f7-1f1f4.png">ğŸ‡·ğŸ‡´</g-emoji>) that come up wayyyy more often than others. For example the word â€œaâ€ comes up significantly more in day to day conversation than â€œwizardâ€ <g-emoji class="g-emoji" alias="tophat" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3a9.png">ğŸ©</g-emoji>. Just how the world works <g-emoji class="g-emoji" alias="earth_americas" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f30e.png">ğŸŒ</g-emoji> With that in mind, knowing how often in comparison one key shows up vs a different key is <strong>critical</strong> to seeming more realistic <g-emoji class="g-emoji" alias="blush" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f60a.png">ğŸ˜Š</g-emoji> This is known as taking the <strong>weighted distribution</strong> into account when deciding what the next <strong>step</strong> should be in the Markov Model.</p>
<p>One way to programmatically <g-emoji class="g-emoji" alias="computer" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4bb.png">ğŸ’»</g-emoji> represent this would be for each <strong>key</strong> that follows a <strong>window</strong> you store the <strong>keys</strong> and the amount of <strong>occurrences</strong> of that key! This can be done via having a dictionary and the dictionary <em>key</em> would represent the <strong>current window</strong> and then have the <em>value</em> of that <em>dictionary key</em> be another dictionary that store the <strong>unique tokens</strong> that follow as <em>keys</em> and their <em>values</em> would be the **amount of occurrencesâ€¦**Does this remind you of something we already talked about <g-emoji class="g-emoji" alias="bar_chart" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4ca.png">ğŸ“Š</g-emoji>? <strong>Histograms</strong>! Exactly! The <em>inner dictionary</em> is severing as a histogram - it is soley keeping track of <strong>keys</strong> and their <strong>occurrences</strong>! Wow, ok so many keys <g-emoji class="g-emoji" alias="key" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f511.png">ğŸ”‘</g-emoji> were brought up and dictionaries too if you are curious about the code you should certainly check it out below<g-emoji class="g-emoji" alias="point_down" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f447.png">ğŸ‘‡</g-emoji> But otherwise, just recognize that in order to create a more advanced model we need to track what <strong>keys</strong> proceed other <strong>keys</strong> and the <strong>amount of occurrences</strong> of these keys.</p>
<p><strong>2. Distribution</strong> | Awesome, quick tangent and then we will start tearing into this example <g-emoji class="g-emoji" alias="yum" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f60b.png">ğŸ˜‹</g-emoji><g-emoji class="g-emoji" alias="fork_and_knife" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f374.png">ğŸ´</g-emoji> Cool so even this data set is <em><strong>very</strong></em> small to be a good corpus! Why? Well, we will get different distribution of words which is great and will impact the entire structure, but in the larger scope of generating <strong>natural</strong> <strong>unique</strong> generated sentences you should aim to have at <strong>minimum 20,000</strong> tokens. It would be better if you would have at least <strong>100,000,</strong> tokens. Then if you want to have a truly spectacular model you should aim for <strong>500,000</strong>+ tokens <g-emoji class="g-emoji" alias="rocket" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f680.png">ğŸš€</g-emoji></p>
<p>But lets chat about how the distribution of words are in a <strong>one key window</strong> with this larger example.</p>
<p><a target="_blank" rel="noopener noreferrer" href="From%20%E2%80%9CWhat%20is%20a%20Markov%20Model%E2%80%9D%20to%20%E2%80%9CHere%20is%20how%20Mark%20ec22e4d1d9d54a94b2f50cfe87242a8d/1e8mGc21E4cK5A-gixEoFoA.png"><img src="From%20%E2%80%9CWhat%20is%20a%20Markov%20Model%E2%80%9D%20to%20%E2%80%9CHere%20is%20how%20Mark%20ec22e4d1d9d54a94b2f50cfe87242a8d/1e8mGc21E4cK5A-gixEoFoA.png" alt="From%20%E2%80%9CWhat%20is%20a%20Markov%20Model%E2%80%9D%20to%20%E2%80%9CHere%20is%20how%20Mark%20ec22e4d1d9d54a94b2f50cfe87242a8d/1e8mGc21E4cK5A-gixEoFoA.png" style="max-width: 100%;"></a></p>
<p>Colored distribution of keys</p>
<p>Wow! Very cool <g-emoji class="g-emoji" alias="sunglasses" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f60e.png">ğŸ˜</g-emoji> Look at all that data - I went ahead and cleaned the data up and now you can see that each unique <strong>key</strong> in our corpus has an array of all of the <strong>keys</strong> and <strong>occurrences</strong> that follow the unique key. Very nice!</p>
<p>Want to know a little secret? <g-emoji class="g-emoji" alias="smirk" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f60f.png">ğŸ˜</g-emoji> There is very little difference between this and the previous Markov model because in both situations we make decisions on the next step <strong>solely based on the current status</strong> but storing the distribution of words allows us to weight the next step. Lets look at a real example from our data:</p>
<pre><code>more : [things : 1, places : 1, that : 2]
</code></pre>
<p>Awesome! <g-emoji class="g-emoji" alias="clap" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f44f.png">ğŸ‘</g-emoji> So if the Markov Modelâ€™s current status was â€œmoreâ€ than we would randomly select one of the following words: â€œthingsâ€, â€œplacesâ€, and â€œthatâ€. However, â€œthatâ€ appears twice as opposed to â€œthingsâ€ and â€œplacesâ€ which occur once. Therefore, there is a 50% chance â€œthatâ€ would be selected and a 25% that either â€œthingsâ€ or â€œplacesâ€ is selected! <g-emoji class="g-emoji" alias="smiley" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f603.png">ğŸ˜ƒ</g-emoji></p>
<pre><code>think : [high : 1, up : 1, right : 1, low : 1, left : 1]
</code></pre>
<p><g-emoji class="g-emoji" alias="point_up" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/261d.png">â˜ï¸</g-emoji><g-emoji class="g-emoji" alias="point_up" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/261d.png">â˜ï¸</g-emoji><g-emoji class="g-emoji" alias="point_up" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/261d.png">â˜ï¸</g-emoji> Awesome, similar example as above, but in this case â€œhighâ€, â€œupâ€, â€œrightâ€, â€œlowâ€, and â€œleftâ€ all have a 20% chance of being selected as the next state if â€œthinkâ€ is the current state! Make sense? <g-emoji class="g-emoji" alias="thought_balloon" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4ad.png">ğŸ’­</g-emoji></p>
<p><strong>3. Bigger Windows</strong> | Currently, we have only been looking at markov models with <em>windows</em> of size one. We could increase the size of the window to get more <strong>â€œaccurateâ€ sentences</strong>. By more accurate I mean there will be less randomness in the generated sentences by the model because they will be closer and closer to the original corpus sentences. This can be good or bad <g-emoji class="g-emoji" alias="innocent" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f607.png">ğŸ˜‡</g-emoji> <g-emoji class="g-emoji" alias="smiling_imp" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f608.png">ğŸ˜ˆ</g-emoji> This is because if your purpose of the Markov Model is to generate some truly unqiue random sentences it would need to be a smaller window. A larger window is only a good idea if you have a <em><strong>significantly</strong></em> large corpus <strong>100,000+ tokens.</strong></p>
<p>Increasing the size of the window is known as bringing the Markov Model to a <strong>â€œhigher orderâ€</strong>. The current examples we have worked with have been <strong>first order markov models.</strong> If we use a second order Markov Model our window size would be two! Similarly, for a third order â†’ window size of three.</p>
<p>The window is the data in the current state of the Markov Model and is what is used for decision making. If there is a bigger window in a smaller data set it is unlikely that there will be large unique distributions for the possible outcomes from one window therefore it could <strong>only recreate the same sentences.</strong></p>
<p>Letâ€™s look at our original example with a <strong>second order Markov Model</strong> - window of size two! <g-emoji class="g-emoji" alias="two" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/0032-20e3.png">2ï¸âƒ£</g-emoji></p>
<p><a target="_blank" rel="noopener noreferrer" href="From%20%E2%80%9CWhat%20is%20a%20Markov%20Model%E2%80%9D%20to%20%E2%80%9CHere%20is%20how%20Mark%20ec22e4d1d9d54a94b2f50cfe87242a8d/1PXimcZKB-1y82tQJNTZ94g.png"><img src="From%20%E2%80%9CWhat%20is%20a%20Markov%20Model%E2%80%9D%20to%20%E2%80%9CHere%20is%20how%20Mark%20ec22e4d1d9d54a94b2f50cfe87242a8d/1PXimcZKB-1y82tQJNTZ94g.png" alt="From%20%E2%80%9CWhat%20is%20a%20Markov%20Model%E2%80%9D%20to%20%E2%80%9CHere%20is%20how%20Mark%20ec22e4d1d9d54a94b2f50cfe87242a8d/1PXimcZKB-1y82tQJNTZ94g.png" style="max-width: 100%;"></a></p>
<p>2nd order Markov Model Data</p>
<p>Very interesting! Any observations? <g-emoji class="g-emoji" alias="detective" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f575.png">ğŸ•µï¸</g-emoji> You may have noticed that every unique window of size two only has <strong>one possible outcome</strong>â€¦therefore no matter where we start we will always get the same sentence because there is no possibility of deviating off the original path. There is a 100% chance we generate the same sentence <g-emoji class="g-emoji" alias="-1" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f44e.png">ğŸ‘</g-emoji> Not great. This reveals a potential issue you can face with Markov Modelsâ€¦if you do not have a large enough corpus you will likely only generate sentences within the corpus which is not generating anything unique. Get a <strong>huge data set - 500,000+ tokens</strong> and then <strong>play</strong> <strong>around</strong> with using <strong>different orders</strong> of the Markov Model <g-emoji class="g-emoji" alias="+1" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f44d.png">ğŸ‘</g-emoji></p>
<h3><a id="user-content-implementation--python-" class="anchor" aria-hidden="true" href="#implementation--python-"><span aria-hidden="true" class="octicon octicon-link"></span></a>Implementation â€” Python <g-emoji class="g-emoji" alias="snake" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f40d.png">ğŸ</g-emoji></h3>
<p><strong>1. Dictogram Data Structure</strong> |
The Dictogram purpose of the Dictogram is to act as a histogram but have incredibly fast <g-emoji class="g-emoji" alias="dash" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4a8.png">ğŸ’¨</g-emoji> and constant look up times regardless how large our data set gets. Basically it is a histogram built using a dictionary because dictionaries has the unique property of having constant lookup time O(1)!</p>
<p>The dictogram class can be created with an iterable data set, such as a list of words or entire books. I keep track of <strong>token</strong> and <strong>key</strong> count as I create it just so I can access those values without having to go through the entire data set <g-emoji class="g-emoji" alias="nerd_face" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f913.png">ğŸ¤“</g-emoji></p>
<p>It is also good to note that I made two functions to return a random word. One just <strong>picks a random key</strong> and the other function takes into account the amount of occurrences for each word and then returns a <strong>weighted random word!</strong> <strong><g-emoji class="g-emoji" alias="weight_lifting_woman" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f3cb-2640.png">ğŸ‹ï¸â€â™€ï¸</g-emoji></strong></p>
<p><strong>2. Markov Model Structure</strong> |
Wow! It has been quite a journey to go from <em><strong>what</strong></em> is a Markov Model to now be talking about <em><strong>how to</strong></em> implement a Markov Model <g-emoji class="g-emoji" alias="sunrise_over_mountains" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f304.png">ğŸŒ„</g-emoji></p>
<p><a href="From%20%E2%80%9CWhat%20is%20a%20Markov%20Model%E2%80%9D%20to%20%E2%80%9CHere%20is%20how%20Mark%20ec22e4d1d9d54a94b2f50cfe87242a8d/13122645">From%20%E2%80%9CWhat%20is%20a%20Markov%20Model%E2%80%9D%20to%20%E2%80%9CHere%20is%20how%20Mark%20ec22e4d1d9d54a94b2f50cfe87242a8d/13122645</a></p>
<p>In my implementation I have a <strong>dictionary</strong> that stores <em>windows</em> as the <strong>key in the key-value pair</strong> and then the <em>value</em> for each <em>key</em> is a <strong>dictogram</strong>. Basically I store a histogram of words for each window so I know what the next state can be based on a current state <g-emoji class="g-emoji" alias="relieved" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f60c.png">ğŸ˜Œ</g-emoji> We increment the data in the dictogram for a key if it already exists in the current window!</p>
<p>**3. Nth Order Markov Model Structure |**Some of you are definitely curious about how to <strong>implement higher order Markov Models</strong> so I also included how I went about doing that <g-emoji class="g-emoji" alias="smirk" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f60f.png">ğŸ˜</g-emoji></p>
<p><g-emoji class="g-emoji" alias="point_up" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/261d.png">â˜ï¸</g-emoji><g-emoji class="g-emoji" alias="point_up" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/261d.png">â˜ï¸</g-emoji><g-emoji class="g-emoji" alias="point_up" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/261d.png">â˜ï¸</g-emoji><g-emoji class="g-emoji" alias="point_up" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/261d.png">â˜ï¸</g-emoji><g-emoji class="g-emoji" alias="point_up" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/261d.png">â˜ï¸</g-emoji> Very similar to the first order Markov Model, but in this case we store a <em><strong>tuple</strong></em> as the <strong>key</strong> in the <strong>key-value pair</strong> in the <strong>dictionary</strong>. We do this because a <em><strong>tuple</strong></em> is a great way to represent a single list. And we use a tuple instead of a list because a key in a dictionary should not change and tuples are immutable sooo <g-emoji class="g-emoji" alias="man_shrugging" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f937-2642.png">ğŸ¤·â€â™‚ï¸</g-emoji></p>
<p><strong>4. Parse Markov Model</strong> |
Yay!! <g-emoji class="g-emoji" alias="tada" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f389.png">ğŸ‰</g-emoji><g-emoji class="g-emoji" alias="clap" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f44f.png">ğŸ‘</g-emoji> <g-emoji class="g-emoji" alias="tada" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f389.png">ğŸ‰</g-emoji> We now have implemented a dictogram, but how do we now do the thing where we generate content based on <strong>current status</strong> and <strong>step</strong> to a new state? <g-emoji class="g-emoji" alias="point_down" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f447.png">ğŸ‘‡</g-emoji><g-emoji class="g-emoji" alias="point_down" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f447.png">ğŸ‘‡</g-emoji><g-emoji class="g-emoji" alias="point_down" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f447.png">ğŸ‘‡</g-emoji>Here we will walk through our model <g-emoji class="g-emoji" alias="walking" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f6b6.png">ğŸš¶</g-emoji></p>
<p>Great, so I <em>personally</em> wanted to be able to only use valid starting sentence words so I checked anything in the <strong>END</strong> key dictogram <g-emoji class="g-emoji" alias="dog" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f436.png">ğŸ¶</g-emoji>. Otherwise, you start the generated data with a <strong>starting state</strong> (which I generate from valid starts), then you just keep looking at the <strong>possible keys</strong> (by going into the dictogram for that key) that could <strong>follow the current state</strong> and <strong>make a decision</strong> based on <em>probability</em> and <em>randomness</em> (<strong>weighted probability</strong>). We keep repeating this until we do it <em><strong>length</strong></em> times! <g-emoji class="g-emoji" alias="100" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f4af.png">ğŸ’¯</g-emoji></p>
<h3><a id="user-content-further-readings-suggestions-thoughts-" class="anchor" aria-hidden="true" href="#further-readings-suggestions-thoughts-"><span aria-hidden="true" class="octicon octicon-link"></span></a>Further Readings, Suggestions, Thoughts <g-emoji class="g-emoji" alias="green_heart" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f49a.png">ğŸ’š</g-emoji></h3>
<p><strong>1. Applications</strong> | Some classic examples of Markov models include peoples actions based on weather, the stock market, and tweet generators! <g-emoji class="g-emoji" alias="hatching_chick" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f423.png">ğŸ£</g-emoji> Think about how you could use a <strong>corpus</strong> to create and <em><strong>generate new content</strong></em> based on a <strong>Markov Model</strong>. Think about what would change?
Hint: Not too much, if you have a solid understanding of what, why, and how Markov Models work and can be created the <strong>only difference</strong> will be <em>how you parse the Markov Model</em> and if you add any <em>unique restrictions.</em></p>
<p>For example, in my dope <strong><a href="http://bit.ly/SVTweets" rel="nofollow">silicon valley tweet generator</a></strong> I used a larger window, limited all my generated content to be less than 140 character, there could be a variable amount of sentences, and I used only existing sentence starting windows to â€œseedâ€ the sentences. <g-emoji class="g-emoji" alias="seedling" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f331.png">ğŸŒ±</g-emoji></p>
<p><strong>2. Further Reading</strong> | Now that you have a good understanding of what a Markov Model is maybe you could explore how a Hidden Markov Model works. Or maybe if you are more inclined to build something using your new found knowledge you could read my artcile on building a HBO Silicon Valley Tweet Generator using a markov model (coming soon) !</p>
<p><strong>3. Final Thoughts</strong> | I am always looking for feedback so please feel free to share your thoughts on how the article was structured, the content, examples, or anything else you want to share with me <g-emoji class="g-emoji" alias="blush" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f60a.png">ğŸ˜Š</g-emoji> Markov Models are great tools and I encourage you to build something using oneâ€¦maybe even your own tweet generator <g-emoji class="g-emoji" alias="stuck_out_tongue_winking_eye" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f61c.png">ğŸ˜œ</g-emoji> Cheers! <g-emoji class="g-emoji" alias="hugs" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f917.png">ğŸ¤—</g-emoji></p>
<p><em>If you liked this article, click the<g-emoji class="g-emoji" alias="green_heart" fallback-src="https://github.githubassets.com/images/icons/emoji/unicode/1f49a.png">ğŸ’š</g-emoji> below so other people will see it here on Medium.</em></p>
<p><a target="_blank" rel="noopener noreferrer" href="From%20%E2%80%9CWhat%20is%20a%20Markov%20Model%E2%80%9D%20to%20%E2%80%9CHere%20is%20how%20Mark%20ec22e4d1d9d54a94b2f50cfe87242a8d/1fKzgMQaOI_ox1w-PzkYvUQ.png"><img src="From%20%E2%80%9CWhat%20is%20a%20Markov%20Model%E2%80%9D%20to%20%E2%80%9CHere%20is%20how%20Mark%20ec22e4d1d9d54a94b2f50cfe87242a8d/1fKzgMQaOI_ox1w-PzkYvUQ.png" alt="From%20%E2%80%9CWhat%20is%20a%20Markov%20Model%E2%80%9D%20to%20%E2%80%9CHere%20is%20how%20Mark%20ec22e4d1d9d54a94b2f50cfe87242a8d/1fKzgMQaOI_ox1w-PzkYvUQ.png" style="max-width: 100%;"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="From%20%E2%80%9CWhat%20is%20a%20Markov%20Model%E2%80%9D%20to%20%E2%80%9CHere%20is%20how%20Mark%20ec22e4d1d9d54a94b2f50cfe87242a8d/1fSiuSUFIo6lGt2XKJWyqgg.png"><img src="From%20%E2%80%9CWhat%20is%20a%20Markov%20Model%E2%80%9D%20to%20%E2%80%9CHere%20is%20how%20Mark%20ec22e4d1d9d54a94b2f50cfe87242a8d/1fSiuSUFIo6lGt2XKJWyqgg.png" alt="From%20%E2%80%9CWhat%20is%20a%20Markov%20Model%E2%80%9D%20to%20%E2%80%9CHere%20is%20how%20Mark%20ec22e4d1d9d54a94b2f50cfe87242a8d/1fSiuSUFIo6lGt2XKJWyqgg.png" style="max-width: 100%;"></a></p>
<p><a target="_blank" rel="noopener noreferrer" href="From%20%E2%80%9CWhat%20is%20a%20Markov%20Model%E2%80%9D%20to%20%E2%80%9CHere%20is%20how%20Mark%20ec22e4d1d9d54a94b2f50cfe87242a8d/1_1ZNPfn7_5dSLRg4Eu8Bcw.png"><img src="From%20%E2%80%9CWhat%20is%20a%20Markov%20Model%E2%80%9D%20to%20%E2%80%9CHere%20is%20how%20Mark%20ec22e4d1d9d54a94b2f50cfe87242a8d/1_1ZNPfn7_5dSLRg4Eu8Bcw.png" alt="From%20%E2%80%9CWhat%20is%20a%20Markov%20Model%E2%80%9D%20to%20%E2%80%9CHere%20is%20how%20Mark%20ec22e4d1d9d54a94b2f50cfe87242a8d/1_1ZNPfn7_5dSLRg4Eu8Bcw.png" style="max-width: 100%;"></a></p>

        </body>
    </html>
    